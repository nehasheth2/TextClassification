{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nehasheth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nehasheth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nehasheth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "os.chdir(\"/Users/nehasheth/Desktop/UIUC/Sem 3/Text Mining/Assignment 2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Loading the dataset\n",
    "- Import the training set and test set (csv files) \n",
    "- List down the number of reviews in the training set and the test set.\n",
    "- Remove blank reviews in both sets.\n",
    "- List down the number of reviews in the training set and the test set after removing blank reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/21/2023 16:42</td>\n",
       "      <td>Much more accessible for blind users than the ...</td>\n",
       "      <td>Up to this point I?€?ve mostly been using Chat...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/11/2023 12:24</td>\n",
       "      <td>Much anticipated, wasn?€?t let down.</td>\n",
       "      <td>I?€?ve been a user since it?€?s initial roll o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/19/2023 10:16</td>\n",
       "      <td>Almost 5 stars, but?€? no search function</td>\n",
       "      <td>This app would almost be perfect if it wasn?€?...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/27/2023 21:57</td>\n",
       "      <td>4.5 stars, here?€?s why</td>\n",
       "      <td>I recently downloaded the app and overall, it'...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/9/2023 7:49</td>\n",
       "      <td>Good, but Siri support would take it to the ne...</td>\n",
       "      <td>I appreciate the devs implementing Siri suppor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date                                              title  \\\n",
       "0  5/21/2023 16:42  Much more accessible for blind users than the ...   \n",
       "1  7/11/2023 12:24               Much anticipated, wasn?€?t let down.   \n",
       "2  5/19/2023 10:16          Almost 5 stars, but?€? no search function   \n",
       "3  5/27/2023 21:57                            4.5 stars, here?€?s why   \n",
       "4    6/9/2023 7:49  Good, but Siri support would take it to the ne...   \n",
       "\n",
       "                                              review  rating  \n",
       "0  Up to this point I?€?ve mostly been using Chat...       4  \n",
       "1  I?€?ve been a user since it?€?s initial roll o...       4  \n",
       "2  This app would almost be perfect if it wasn?€?...       4  \n",
       "3  I recently downloaded the app and overall, it'...       4  \n",
       "4  I appreciate the devs implementing Siri suppor...       4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"chatgpt_train.csv\")\n",
    "test = pd.read_csv(\"chatgpt_test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.631952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.602059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rating\n",
       "count  1834.000000\n",
       "mean      3.631952\n",
       "std       1.602059\n",
       "min       1.000000\n",
       "25%       2.000000\n",
       "50%       4.000000\n",
       "75%       5.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1834 entries, 0 to 1833\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    1834 non-null   object\n",
      " 1   title   1834 non-null   object\n",
      " 2   review  1829 non-null   object\n",
      " 3   rating  1834 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 57.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in the training set: 1834\n",
      "Number of reviews in the test set: 458\n",
      "Number of reviews in the training set after removing blank reviews: 1829\n",
      "Number of reviews in the test set after removing blank reviews: 458\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of reviews in the training set: {len(train)}')\n",
    "print(f'Number of reviews in the test set: {len(test)}')\n",
    "\n",
    "# Remove blank reviews in both sets\n",
    "train_df = train.dropna(subset=['review'])\n",
    "test_df = test.dropna(subset=['review'])\n",
    "\n",
    "print(f'Number of reviews in the training set after removing blank reviews: {len(train_df)}')\n",
    "print(f'Number of reviews in the test set after removing blank reviews: {len(test_df)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : POS Tagging\n",
    "- Make a copy of the training data for Task 2. Implement the below steps on this copy to refrain from editing the actual training data that will be used for further tasks.\n",
    "- Using a package of your choice (e.g., NLTK in Python), perform part-of-speech (POS) tagging.\n",
    "- Explain what parts of speech could be useful for sentiment analysis and why?\n",
    "- Report the POS-tagging results for 3 examples in the dataset. Discuss errors that you observe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1829, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy = train_df.copy()\n",
    "train_copy.shape #copy of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  Up to this point I?€?ve mostly been using Chat...   \n",
      "1  I?€?ve been a user since it?€?s initial roll o...   \n",
      "2  This app would almost be perfect if it wasn?€?...   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(Up, RB), (to, TO), (this, DT), (point, NN), ...  \n",
      "1  [(I, PRP), (?, .), (€, VB), (?, .), (ve, FW), ...  \n",
      "2  [(This, DT), (app, NN), (would, MD), (almost, ...  \n"
     ]
    }
   ],
   "source": [
    "# Apply POS tagging to the 'review' column of the DataFrame\n",
    "train_copy['pos_tags'] = train_copy['review'].apply(pos_tagging)\n",
    "\n",
    "# Display the first few rows of the DataFrame with POS tags\n",
    "print(train_copy[['review', 'pos_tags']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'title', 'review', 'rating', 'pos_tags'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Before POS tagging:\n",
      "Up to this point I?€?ve mostly been using ChatGPT on my windows desktop using Google Chrome. While it?€?s doable, screen reader navigation is pretty difficult on the desktop site and you really have to be an advanced user to find your way through it. I have submitted numerous feedbacks to open AI about this but nothing has changed on that front.\n",
      "Well, the good news ?€? the iOS app pretty much addresses all of those problems. The UI seems really clean, uncluttered and designed well to be compatible with voiceover, the screen reader built into iOS. I applaud the inclusivity of this design ?€? I only wish they would give the same attention and care to the accessibility experience of the desktop app.\n",
      "I would have given this review five stars but I have just a couple minor quibbles. First, once I submit my prompt, voiceover starts to read aloud ChatGPT?€?s response before that response is finished, so I will hear the first few words of the response followed by voiceover reading aloud the ?€?stop generating?€? button, which isn?€?t super helpful. It would be great if you could better coordinate this alert so that it didn?€?t start reading the message until it had been fully generated. The other thing I?€?d like is a Feedback button easily accessible from within the main screen of the app, to make it as easy as possible to get continuing suggestions and feedback from your users.\n",
      "Otherwise, fantastic app so far!\n",
      "After POS tagging:\n",
      "[('Up', 'RB'), ('to', 'TO'), ('this', 'DT'), ('point', 'NN'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('ve', 'FW'), ('mostly', 'RB'), ('been', 'VBN'), ('using', 'VBG'), ('ChatGPT', 'NNP'), ('on', 'IN'), ('my', 'PRP$'), ('windows', 'NNS'), ('desktop', 'VBP'), ('using', 'VBG'), ('Google', 'NNP'), ('Chrome', 'NNP'), ('.', '.'), ('While', 'IN'), ('it', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('s', 'NN'), ('doable', 'JJ'), (',', ','), ('screen', 'JJ'), ('reader', 'NN'), ('navigation', 'NN'), ('is', 'VBZ'), ('pretty', 'RB'), ('difficult', 'JJ'), ('on', 'IN'), ('the', 'DT'), ('desktop', 'NN'), ('site', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('really', 'RB'), ('have', 'VB'), ('to', 'TO'), ('be', 'VB'), ('an', 'DT'), ('advanced', 'JJ'), ('user', 'NN'), ('to', 'TO'), ('find', 'VB'), ('your', 'PRP$'), ('way', 'NN'), ('through', 'IN'), ('it', 'PRP'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('submitted', 'VBN'), ('numerous', 'JJ'), ('feedbacks', 'NNS'), ('to', 'TO'), ('open', 'VB'), ('AI', 'NNP'), ('about', 'IN'), ('this', 'DT'), ('but', 'CC'), ('nothing', 'NN'), ('has', 'VBZ'), ('changed', 'VBN'), ('on', 'IN'), ('that', 'DT'), ('front', 'NN'), ('.', '.'), ('Well', 'UH'), (',', ','), ('the', 'DT'), ('good', 'JJ'), ('news', 'NN'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('the', 'DT'), ('iOS', 'NN'), ('app', 'VBZ'), ('pretty', 'RB'), ('much', 'JJ'), ('addresses', 'VBZ'), ('all', 'DT'), ('of', 'IN'), ('those', 'DT'), ('problems', 'NNS'), ('.', '.'), ('The', 'DT'), ('UI', 'NNP'), ('seems', 'VBZ'), ('really', 'RB'), ('clean', 'JJ'), (',', ','), ('uncluttered', 'JJ'), ('and', 'CC'), ('designed', 'VBN'), ('well', 'RB'), ('to', 'TO'), ('be', 'VB'), ('compatible', 'JJ'), ('with', 'IN'), ('voiceover', 'NN'), (',', ','), ('the', 'DT'), ('screen', 'NN'), ('reader', 'NN'), ('built', 'VBN'), ('into', 'IN'), ('iOS', 'NN'), ('.', '.'), ('I', 'PRP'), ('applaud', 'VBP'), ('the', 'DT'), ('inclusivity', 'NN'), ('of', 'IN'), ('this', 'DT'), ('design', 'NN'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('I', 'PRP'), ('only', 'RB'), ('wish', 'VBD'), ('they', 'PRP'), ('would', 'MD'), ('give', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('attention', 'NN'), ('and', 'CC'), ('care', 'NN'), ('to', 'TO'), ('the', 'DT'), ('accessibility', 'NN'), ('experience', 'NN'), ('of', 'IN'), ('the', 'DT'), ('desktop', 'NN'), ('app', 'NN'), ('.', '.'), ('I', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('given', 'VBN'), ('this', 'DT'), ('review', 'NN'), ('five', 'CD'), ('stars', 'NNS'), ('but', 'CC'), ('I', 'PRP'), ('have', 'VBP'), ('just', 'RB'), ('a', 'DT'), ('couple', 'NN'), ('minor', 'JJ'), ('quibbles', 'NNS'), ('.', '.'), ('First', 'NNP'), (',', ','), ('once', 'RB'), ('I', 'PRP'), ('submit', 'VBP'), ('my', 'PRP$'), ('prompt', 'NN'), (',', ','), ('voiceover', 'NN'), ('starts', 'NNS'), ('to', 'TO'), ('read', 'VB'), ('aloud', 'JJ'), ('ChatGPT', 'NNP'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('s', 'JJ'), ('response', 'NN'), ('before', 'IN'), ('that', 'DT'), ('response', 'NN'), ('is', 'VBZ'), ('finished', 'VBN'), (',', ','), ('so', 'IN'), ('I', 'PRP'), ('will', 'MD'), ('hear', 'VB'), ('the', 'DT'), ('first', 'JJ'), ('few', 'JJ'), ('words', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('response', 'NN'), ('followed', 'VBN'), ('by', 'IN'), ('voiceover', 'NN'), ('reading', 'NN'), ('aloud', 'IN'), ('the', 'DT'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('stop', 'VB'), ('generating', 'VBG'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('button', 'NN'), (',', ','), ('which', 'WDT'), ('isn', 'VBZ'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('t', 'JJ'), ('super', 'JJ'), ('helpful', 'NN'), ('.', '.'), ('It', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('great', 'JJ'), ('if', 'IN'), ('you', 'PRP'), ('could', 'MD'), ('better', 'VB'), ('coordinate', 'VB'), ('this', 'DT'), ('alert', 'NN'), ('so', 'IN'), ('that', 'IN'), ('it', 'PRP'), ('didn', 'VB'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('t', 'JJ'), ('start', 'NN'), ('reading', 'VBG'), ('the', 'DT'), ('message', 'NN'), ('until', 'IN'), ('it', 'PRP'), ('had', 'VBD'), ('been', 'VBN'), ('fully', 'RB'), ('generated', 'VBN'), ('.', '.'), ('The', 'DT'), ('other', 'JJ'), ('thing', 'NN'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('d', 'NN'), ('like', 'IN'), ('is', 'VBZ'), ('a', 'DT'), ('Feedback', 'NNP'), ('button', 'NN'), ('easily', 'RB'), ('accessible', 'JJ'), ('from', 'IN'), ('within', 'IN'), ('the', 'DT'), ('main', 'JJ'), ('screen', 'NN'), ('of', 'IN'), ('the', 'DT'), ('app', 'NN'), (',', ','), ('to', 'TO'), ('make', 'VB'), ('it', 'PRP'), ('as', 'RB'), ('easy', 'JJ'), ('as', 'IN'), ('possible', 'JJ'), ('to', 'TO'), ('get', 'VB'), ('continuing', 'VBG'), ('suggestions', 'NNS'), ('and', 'CC'), ('feedback', 'NN'), ('from', 'IN'), ('your', 'PRP$'), ('users', 'NNS'), ('.', '.'), ('Otherwise', 'RB'), (',', ','), ('fantastic', 'JJ'), ('app', 'NN'), ('so', 'RB'), ('far', 'RB'), ('!', '.')]\n",
      "----------------------------------------\n",
      "Example 2:\n",
      "Before POS tagging:\n",
      "I?€?ve been a user since it?€?s initial roll out and have been waiting for a mobile application ever since using the web app. For reference  I?€?m a software engineering student while working in IT full time. I have to say GPT is an crucial tool. It takes far less time to get information quickly that you?€?d otherwise have to source from stack-overflow, various red-hat articles, Ubuntu articles, searching through software documentation, Microsoft documentation ect. Typically chat gpt can find the answer in a fraction of a second that google can. Obviously it is wrong, a lot. But to have the ability to get quick information on my phone like I can in the web browser I?€?m super excited about and have already been using the mobile app since download constantly. And I?€?m excited for the future of this program becoming more accurate and it seems to be getting more and more precise with every roll out. Gone are the days scouring the internet for obscure pieces of information, chat gpt can find it for you with 2 or 3 prompts. I love this app and I?€?m so happy it?€?s on mobile now. The UI is also very sleek, easy to use. My only complaint with the interface is the history tab at the top right. I actually prefer the conversation tabs on the left in the web app but I understand it would make the app kind of clunky especially on mobile since the screen size is smaller. Anyway, awesome app 5 stars.\n",
      "After POS tagging:\n",
      "[('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('ve', 'FW'), ('been', 'VBN'), ('a', 'DT'), ('user', 'NN'), ('since', 'IN'), ('it', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('s', 'JJ'), ('initial', 'JJ'), ('roll', 'NN'), ('out', 'RB'), ('and', 'CC'), ('have', 'VBP'), ('been', 'VBN'), ('waiting', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('mobile', 'JJ'), ('application', 'NN'), ('ever', 'RB'), ('since', 'IN'), ('using', 'VBG'), ('the', 'DT'), ('web', 'NN'), ('app', 'NN'), ('.', '.'), ('For', 'IN'), ('reference', 'NN'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('m', 'VB'), ('a', 'DT'), ('software', 'NN'), ('engineering', 'NN'), ('student', 'NN'), ('while', 'IN'), ('working', 'VBG'), ('in', 'IN'), ('IT', 'NNP'), ('full', 'JJ'), ('time', 'NN'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('say', 'VB'), ('GPT', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('crucial', 'JJ'), ('tool', 'NN'), ('.', '.'), ('It', 'PRP'), ('takes', 'VBZ'), ('far', 'RB'), ('less', 'JJR'), ('time', 'NN'), ('to', 'TO'), ('get', 'VB'), ('information', 'NN'), ('quickly', 'RB'), ('that', 'IN'), ('you', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('d', 'JJ'), ('otherwise', 'RB'), ('have', 'VBP'), ('to', 'TO'), ('source', 'NN'), ('from', 'IN'), ('stack-overflow', 'JJ'), (',', ','), ('various', 'JJ'), ('red-hat', 'JJ'), ('articles', 'NNS'), (',', ','), ('Ubuntu', 'NNP'), ('articles', 'NNS'), (',', ','), ('searching', 'VBG'), ('through', 'IN'), ('software', 'NN'), ('documentation', 'NN'), (',', ','), ('Microsoft', 'NNP'), ('documentation', 'NN'), ('ect', 'NN'), ('.', '.'), ('Typically', 'RB'), ('chat', 'WP'), ('gpt', 'NN'), ('can', 'MD'), ('find', 'VB'), ('the', 'DT'), ('answer', 'NN'), ('in', 'IN'), ('a', 'DT'), ('fraction', 'NN'), ('of', 'IN'), ('a', 'DT'), ('second', 'JJ'), ('that', 'IN'), ('google', 'NN'), ('can', 'MD'), ('.', '.'), ('Obviously', 'VB'), ('it', 'PRP'), ('is', 'VBZ'), ('wrong', 'JJ'), (',', ','), ('a', 'DT'), ('lot', 'NN'), ('.', '.'), ('But', 'CC'), ('to', 'TO'), ('have', 'VB'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('get', 'VB'), ('quick', 'JJ'), ('information', 'NN'), ('on', 'IN'), ('my', 'PRP$'), ('phone', 'NN'), ('like', 'IN'), ('I', 'PRP'), ('can', 'MD'), ('in', 'IN'), ('the', 'DT'), ('web', 'NN'), ('browser', 'NN'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('m', 'FW'), ('super', 'NN'), ('excited', 'VBN'), ('about', 'IN'), ('and', 'CC'), ('have', 'VBP'), ('already', 'RB'), ('been', 'VBN'), ('using', 'VBG'), ('the', 'DT'), ('mobile', 'JJ'), ('app', 'NN'), ('since', 'IN'), ('download', 'NN'), ('constantly', 'RB'), ('.', '.'), ('And', 'CC'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('m', 'NN'), ('excited', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('this', 'DT'), ('program', 'NN'), ('becoming', 'VBG'), ('more', 'JJR'), ('accurate', 'JJ'), ('and', 'CC'), ('it', 'PRP'), ('seems', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('getting', 'VBG'), ('more', 'JJR'), ('and', 'CC'), ('more', 'RBR'), ('precise', 'JJ'), ('with', 'IN'), ('every', 'DT'), ('roll', 'NN'), ('out', 'RP'), ('.', '.'), ('Gone', 'NN'), ('are', 'VBP'), ('the', 'DT'), ('days', 'NNS'), ('scouring', 'VBG'), ('the', 'DT'), ('internet', 'NN'), ('for', 'IN'), ('obscure', 'JJ'), ('pieces', 'NNS'), ('of', 'IN'), ('information', 'NN'), (',', ','), ('chat', 'WP'), ('gpt', 'VBD'), ('can', 'MD'), ('find', 'VB'), ('it', 'PRP'), ('for', 'IN'), ('you', 'PRP'), ('with', 'IN'), ('2', 'CD'), ('or', 'CC'), ('3', 'CD'), ('prompts', 'NNS'), ('.', '.'), ('I', 'PRP'), ('love', 'VBP'), ('this', 'DT'), ('app', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('m', 'NNS'), ('so', 'RB'), ('happy', 'JJ'), ('it', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('s', 'NN'), ('on', 'IN'), ('mobile', 'NN'), ('now', 'RB'), ('.', '.'), ('The', 'DT'), ('UI', 'NNP'), ('is', 'VBZ'), ('also', 'RB'), ('very', 'RB'), ('sleek', 'JJ'), (',', ','), ('easy', 'JJ'), ('to', 'TO'), ('use', 'VB'), ('.', '.'), ('My', 'PRP$'), ('only', 'JJ'), ('complaint', 'NN'), ('with', 'IN'), ('the', 'DT'), ('interface', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('history', 'NN'), ('tab', 'NN'), ('at', 'IN'), ('the', 'DT'), ('top', 'JJ'), ('right', 'NN'), ('.', '.'), ('I', 'PRP'), ('actually', 'RB'), ('prefer', 'VBP'), ('the', 'DT'), ('conversation', 'NN'), ('tabs', 'NN'), ('on', 'IN'), ('the', 'DT'), ('left', 'NN'), ('in', 'IN'), ('the', 'DT'), ('web', 'NN'), ('app', 'NN'), ('but', 'CC'), ('I', 'PRP'), ('understand', 'VBP'), ('it', 'PRP'), ('would', 'MD'), ('make', 'VB'), ('the', 'DT'), ('app', 'NN'), ('kind', 'NN'), ('of', 'IN'), ('clunky', 'NN'), ('especially', 'RB'), ('on', 'IN'), ('mobile', 'JJ'), ('since', 'IN'), ('the', 'DT'), ('screen', 'NN'), ('size', 'NN'), ('is', 'VBZ'), ('smaller', 'JJR'), ('.', '.'), ('Anyway', 'NNP'), (',', ','), ('awesome', 'JJ'), ('app', 'NN'), ('5', 'CD'), ('stars', 'NNS'), ('.', '.')]\n",
      "----------------------------------------\n",
      "Example 3:\n",
      "Before POS tagging:\n",
      "This app would almost be perfect if it wasn?€?t for ONE little thing: a ?€?search in?€? function. As anyone can imagine, these AI chats can get quuuuite long, and quite lengthy. And sometimes I wanna go into a chat & look up a specific part or parts of that particular chat by using a search function to look up key words within that chat & track down whatever part I was looking for. For example, in a chat, if I had searched way early into the chat ?€?how do lions hunt??€? And say days later, I wanted to revisit that particular response, I wanna be able to go into the actual chat go to a ?€?search in?€? function and be able to type in key words like ?€?lions?€? or ?€?hunt?€? to be able to automatically find that part in the chat instead of having to scroll through a massive chat to find that part. Similar to what you can do in Microsoft Word docs, or even on web browsers. I think the app already kind of has this, but it doesn?€?t work exactly like that. I tested it out, and all it does is, you type in key words, and it shows you that those words are present in the chat or chats, but it doesn?€?t actually track it down or take you to that specific part. If this app can have that? It?€?s an absolute massive perfect winner. Addendum - I also noticed that my phone (iPhone 14 Pro Max) tends to run a little hotter, which I?€?m sure will affect battery life, when using the app. Just wanted to add that too.\n",
      "After POS tagging:\n",
      "[('This', 'DT'), ('app', 'NN'), ('would', 'MD'), ('almost', 'RB'), ('be', 'VB'), ('perfect', 'JJ'), ('if', 'IN'), ('it', 'PRP'), ('wasn', 'VBZ'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('t', 'NN'), ('for', 'IN'), ('ONE', 'NNP'), ('little', 'JJ'), ('thing', 'NN'), (':', ':'), ('a', 'DT'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('search', 'NN'), ('in', 'IN'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('function', 'NN'), ('.', '.'), ('As', 'IN'), ('anyone', 'NN'), ('can', 'MD'), ('imagine', 'VB'), (',', ','), ('these', 'DT'), ('AI', 'NNP'), ('chats', 'NNS'), ('can', 'MD'), ('get', 'VB'), ('quuuuite', 'RB'), ('long', 'RB'), (',', ','), ('and', 'CC'), ('quite', 'RB'), ('lengthy', 'JJ'), ('.', '.'), ('And', 'CC'), ('sometimes', 'RB'), ('I', 'PRP'), ('wan', 'VBP'), ('na', 'RB'), ('go', 'VBP'), ('into', 'IN'), ('a', 'DT'), ('chat', 'NN'), ('&', 'CC'), ('look', 'VB'), ('up', 'RP'), ('a', 'DT'), ('specific', 'JJ'), ('part', 'NN'), ('or', 'CC'), ('parts', 'NNS'), ('of', 'IN'), ('that', 'DT'), ('particular', 'JJ'), ('chat', 'NN'), ('by', 'IN'), ('using', 'VBG'), ('a', 'DT'), ('search', 'NN'), ('function', 'NN'), ('to', 'TO'), ('look', 'VB'), ('up', 'RP'), ('key', 'JJ'), ('words', 'NNS'), ('within', 'IN'), ('that', 'DT'), ('chat', 'NN'), ('&', 'CC'), ('track', 'VB'), ('down', 'RP'), ('whatever', 'WDT'), ('part', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('looking', 'VBG'), ('for', 'IN'), ('.', '.'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('in', 'IN'), ('a', 'DT'), ('chat', 'NN'), (',', ','), ('if', 'IN'), ('I', 'PRP'), ('had', 'VBD'), ('searched', 'VBN'), ('way', 'NN'), ('early', 'RB'), ('into', 'IN'), ('the', 'DT'), ('chat', 'NN'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('how', 'WRB'), ('do', 'JJ'), ('lions', 'NNS'), ('hunt', 'VB'), ('?', '.'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('And', 'CC'), ('say', 'VBP'), ('days', 'NNS'), ('later', 'RB'), (',', ','), ('I', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('revisit', 'VB'), ('that', 'IN'), ('particular', 'JJ'), ('response', 'NN'), (',', ','), ('I', 'PRP'), ('wan', 'VBP'), ('na', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('go', 'VB'), ('into', 'IN'), ('the', 'DT'), ('actual', 'JJ'), ('chat', 'NN'), ('go', 'VBP'), ('to', 'TO'), ('a', 'DT'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('search', 'NN'), ('in', 'IN'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('function', 'NN'), ('and', 'CC'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('type', 'VB'), ('in', 'IN'), ('key', 'JJ'), ('words', 'NNS'), ('like', 'IN'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('lions', 'NNS'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('or', 'CC'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('hunt', 'NN'), ('?', '.'), ('€', 'NN'), ('?', '.'), ('to', 'TO'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('automatically', 'RB'), ('find', 'VB'), ('that', 'DT'), ('part', 'NN'), ('in', 'IN'), ('the', 'DT'), ('chat', 'NN'), ('instead', 'RB'), ('of', 'IN'), ('having', 'VBG'), ('to', 'TO'), ('scroll', 'VB'), ('through', 'IN'), ('a', 'DT'), ('massive', 'JJ'), ('chat', 'NN'), ('to', 'TO'), ('find', 'VB'), ('that', 'DT'), ('part', 'NN'), ('.', '.'), ('Similar', 'JJ'), ('to', 'TO'), ('what', 'WP'), ('you', 'PRP'), ('can', 'MD'), ('do', 'VB'), ('in', 'IN'), ('Microsoft', 'NNP'), ('Word', 'NNP'), ('docs', 'NN'), (',', ','), ('or', 'CC'), ('even', 'RB'), ('on', 'IN'), ('web', 'NN'), ('browsers', 'NNS'), ('.', '.'), ('I', 'PRP'), ('think', 'VBP'), ('the', 'DT'), ('app', 'NN'), ('already', 'RB'), ('kind', 'NN'), ('of', 'IN'), ('has', 'VBZ'), ('this', 'DT'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('doesn', 'VBZ'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('t', 'JJ'), ('work', 'NN'), ('exactly', 'RB'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), ('I', 'PRP'), ('tested', 'VBD'), ('it', 'PRP'), ('out', 'RP'), (',', ','), ('and', 'CC'), ('all', 'DT'), ('it', 'PRP'), ('does', 'VBZ'), ('is', 'VBZ'), (',', ','), ('you', 'PRP'), ('type', 'VBP'), ('in', 'IN'), ('key', 'JJ'), ('words', 'NNS'), (',', ','), ('and', 'CC'), ('it', 'PRP'), ('shows', 'VBZ'), ('you', 'PRP'), ('that', 'IN'), ('those', 'DT'), ('words', 'NNS'), ('are', 'VBP'), ('present', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('chat', 'NN'), ('or', 'CC'), ('chats', 'NNS'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('doesn', 'VBZ'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('t', 'VB'), ('actually', 'RB'), ('track', 'VB'), ('it', 'PRP'), ('down', 'RP'), ('or', 'CC'), ('take', 'VB'), ('you', 'PRP'), ('to', 'TO'), ('that', 'DT'), ('specific', 'JJ'), ('part', 'NN'), ('.', '.'), ('If', 'IN'), ('this', 'DT'), ('app', 'NN'), ('can', 'MD'), ('have', 'VB'), ('that', 'DT'), ('?', '.'), ('It', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('s', 'VB'), ('an', 'DT'), ('absolute', 'JJ'), ('massive', 'JJ'), ('perfect', 'NN'), ('winner', 'NN'), ('.', '.'), ('Addendum', 'NNP'), ('-', ':'), ('I', 'PRP'), ('also', 'RB'), ('noticed', 'VBD'), ('that', 'IN'), ('my', 'PRP$'), ('phone', 'NN'), ('(', '('), ('iPhone', 'JJ'), ('14', 'CD'), ('Pro', 'NNP'), ('Max', 'NNP'), (')', ')'), ('tends', 'VBZ'), ('to', 'TO'), ('run', 'VB'), ('a', 'DT'), ('little', 'JJ'), ('hotter', 'NN'), (',', ','), ('which', 'WDT'), ('I', 'PRP'), ('?', '.'), ('€', 'VB'), ('?', '.'), ('m', 'JJ'), ('sure', 'NN'), ('will', 'MD'), ('affect', 'VB'), ('battery', 'NN'), ('life', 'NN'), (',', ','), ('when', 'WRB'), ('using', 'VBG'), ('the', 'DT'), ('app', 'NN'), ('.', '.'), ('Just', 'RB'), ('wanted', 'VBD'), ('to', 'TO'), ('add', 'VB'), ('that', 'DT'), ('too', 'RB'), ('.', '.')]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataFrame and report examples before and after stemming\n",
    "for i in range(3):\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    \n",
    "    #stopword removal\n",
    "    print(\"Before POS tagging:\")\n",
    "    print(train_copy['review'][i])\n",
    "    print(\"After POS tagging:\")\n",
    "    print(train_copy['pos_tags'][i])\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors observed : \n",
    "In the first example, the following errors can be observed :\n",
    "- Tokenization splits the text into words or tokens. In this example, tokenization appears to treat some punctuation marks (e.g., '€', '?', '€', '?', 's', '...', '€', '?', '€', '?') as separate tokens. These should ideally be part of adjacent words or handled differently.\n",
    "\n",
    "- In the phrase \"it's,\" \"s\" is incorrectly tagged as a noun (NN) when it should be recognized as a contraction of \"is\" (VBZ). The same issue occurs with \"I'm,\" \"I'd,\" and other contractions.\n",
    "\n",
    "- The word \"submit\" is tagged as a noun (NN) when it should be a verb (VB). Similarly, \"starts\" is tagged as a noun (NN) when it should be a verb (VB).\n",
    "\n",
    "-  The Euro symbol '€' is tagged as a noun (NN). This is an error and should be tagged as a special character or symbol."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Useful for Sentiment Analysis:\n",
    "\n",
    "- Adjectives (JJ, JJR, JJS): Positive adjectives like \"great,\" \"excellent,\" and \"amazing\" typically indicate positive sentiment, while negative adjectives like \"poor,\" \"bad,\" and \"terrible\" indicate negative sentiment.\n",
    "- Verbs (VB, VBD, VBG, VBN, VBP, VBZ): Positive verbs like \"love\" and \"enjoy\" can indicate positive sentiment, while negative verbs like \"hate\" and \"dislike\" can indicate negative sentiment.\n",
    "- Nouns (NN, NNS, NNP, NNPS): Nouns can help identify the subject or object of the sentiment. For example, \"product\" or \"service\" can be essential nouns in sentiment analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 : Extract unigram features\n",
    "- Extract unigrams from the review column in the training set. \n",
    "- Fit the unigrams to the review column to generate a feature vector for each review in the training set and testing set. Note that we are using count-based features (i.e., feature values should be the number of times a specific unigram appears in the review). \n",
    "- Report the number of features in the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (unigrams) in the training set: 5551\n",
      "Number of features (unigrams) in the test set: 5551\n"
     ]
    }
   ],
   "source": [
    "train_reviews = train_df['review']\n",
    "test_reviews = test_df['review']\n",
    "\n",
    "# initialize a CountVectorizer for unigrams (single words)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# git and transform the training data to generate feature vectors\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "\n",
    "# transform the testing data using the same vectorizer\n",
    "X_test = vectorizer.transform(test_reviews)\n",
    "\n",
    "# get the feature names (unigrams) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f'Number of features (unigrams) in the training set: {len(feature_names)}')\n",
    "print(f'Number of features (unigrams) in the test set: {X_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '100', '101', ..., 'zoom', 'zoomed', 'zooming'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 : Train and evaluate classifier\n",
    "- With unigram features generated in task 3, train a Naïve Bayes classifier. \n",
    "- After the training, apply the classifier to the test set anlind calculate the following performance metrics: \n",
    "- Overall (average) accuracy, precision, recall and F1 score of the classifier.\n",
    "- Accuracy, precision, recall and F1 score for each label: 1,2,3,4,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6332\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5657    0.4870    0.5234       115\n",
      "           2     0.4000    0.0769    0.1290        26\n",
      "           3     0.1333    0.0800    0.1000        25\n",
      "           4     0.3889    0.1591    0.2258        44\n",
      "           5     0.6947    0.8992    0.7838       248\n",
      "\n",
      "    accuracy                         0.6332       458\n",
      "   macro avg     0.4365    0.3404    0.3524       458\n",
      "weighted avg     0.5855    0.6332    0.5903       458\n",
      "\n",
      "Performance Metrics for Label 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.4870    0.6550       115\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4870       115\n",
      "   macro avg     0.2500    0.8717    0.1637       115\n",
      "weighted avg     1.0000    0.4870    0.6550       115\n",
      "\n",
      "Performance Metrics for Label 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           2     1.0000    0.0769    0.1429        26\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.0769        26\n",
      "   macro avg     0.2000    0.8154    0.0286        26\n",
      "weighted avg     1.0000    0.0769    0.1429        26\n",
      "\n",
      "Performance Metrics for Label 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           3     1.0000    0.0800    0.1481        25\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.0800        25\n",
      "   macro avg     0.2500    0.7700    0.0370        25\n",
      "weighted avg     1.0000    0.0800    0.1481        25\n",
      "\n",
      "Performance Metrics for Label 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     1.0000    0.1591    0.2745        44\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.1591        44\n",
      "   macro avg     0.2500    0.7898    0.0686        44\n",
      "weighted avg     1.0000    0.1591    0.2745        44\n",
      "\n",
      "Performance Metrics for Label 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           2     0.0000    1.0000    0.0000         0\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     1.0000    0.8992    0.9469       248\n",
      "\n",
      "    accuracy                         0.8992       248\n",
      "   macro avg     0.2000    0.9798    0.1894       248\n",
      "weighted avg     1.0000    0.8992    0.9469       248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the target Y\n",
    "train_ratings = train_df['rating']\n",
    "test_ratings = test_df['rating']\n",
    "\n",
    "# initiatize Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_ratings)\n",
    "\n",
    "predicted_ratings = classifier.predict(X_test)\n",
    "\n",
    "# calculate overall performance metrics\n",
    "overall_accuracy = accuracy_score(test_ratings, predicted_ratings)\n",
    "overall_report = classification_report(test_ratings, predicted_ratings, digits=4, zero_division=1)\n",
    "\n",
    "# calculate performance metrics for each label (1, 2, 3, 4, 5)\n",
    "label_reports = {}\n",
    "for label in range(1, 6):\n",
    "    label_indices = (test_ratings == label)\n",
    "    label_predictions = predicted_ratings[label_indices]\n",
    "    label_true_values = test_ratings[label_indices]\n",
    "    label_report = classification_report(label_true_values, label_predictions, digits=4, zero_division=1)\n",
    "    label_reports[label] = label_report\n",
    "\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
    "print('Overall Classification Report:')\n",
    "print(overall_report)\n",
    "\n",
    "for label, label_report in label_reports.items():\n",
    "    print(f'Performance Metrics for Label {label}:')\n",
    "    print(label_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 : Add bigram features\n",
    "- Extract bigrams from the review column in the training set and add these bigrams into the feature space (unigram + bigram features). \n",
    "- Fit the new features to the review column to generate a feature vector for each review in the training set and the test set. Report the number of features of the training set and the test set.\n",
    "- Repeat task 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_reviews \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m train_ratings \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m test_reviews \u001b[39m=\u001b[39m test_df[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_reviews = train_df['review']\n",
    "train_ratings = train_df['rating']\n",
    "test_reviews = test_df['review']\n",
    "test_ratings = test_df['rating']\n",
    "\n",
    "#for unigrams\n",
    "unigram_vectorizer = CountVectorizer()\n",
    "X_train_unigram = unigram_vectorizer.fit_transform(train_reviews) #unigram\n",
    "X_test_unigram = unigram_vectorizer.transform(test_reviews)\n",
    "\n",
    "#for bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_bigram = bigram_vectorizer.fit_transform(train_reviews)\n",
    "X_test_bigram = bigram_vectorizer.transform(test_reviews)\n",
    "\n",
    "# Combine unigram and bigram feature vectors for both training and testing sets\n",
    "X_train_combined = pd.concat([pd.DataFrame(X_train_unigram.toarray()), pd.DataFrame(X_train_bigram.toarray())], axis=1)\n",
    "X_test_combined = pd.concat([pd.DataFrame(X_test_unigram.toarray()), pd.DataFrame(X_test_bigram.toarray())], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6245\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6282    0.4261    0.5078       115\n",
      "           2     1.0000    0.0000    0.0000        26\n",
      "           3     0.0000    0.0000    0.0000        25\n",
      "           4     0.0769    0.0227    0.0351        44\n",
      "           5     0.6519    0.9516    0.7738       248\n",
      "\n",
      "    accuracy                         0.6245       458\n",
      "   macro avg     0.4714    0.2801    0.2633       458\n",
      "weighted avg     0.5749    0.6245    0.5499       458\n",
      "\n",
      "Performance Metrics for Label 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.4261    0.5976       115\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4261       115\n",
      "   macro avg     0.2500    0.8565    0.1494       115\n",
      "weighted avg     1.0000    0.4261    0.5976       115\n",
      "\n",
      "Performance Metrics for Label 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           2     1.0000    0.0000    0.0000      26.0\n",
      "           3     0.0000    1.0000    0.0000       0.0\n",
      "           4     0.0000    1.0000    0.0000       0.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      26.0\n",
      "   macro avg     0.2000    0.8000    0.0000      26.0\n",
      "weighted avg     1.0000    0.0000    0.0000      26.0\n",
      "\n",
      "Performance Metrics for Label 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           3     1.0000    0.0000    0.0000      25.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      25.0\n",
      "   macro avg     0.3333    0.6667    0.0000      25.0\n",
      "weighted avg     1.0000    0.0000    0.0000      25.0\n",
      "\n",
      "Performance Metrics for Label 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     1.0000    0.0227    0.0444        44\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.0227        44\n",
      "   macro avg     0.2500    0.7557    0.0111        44\n",
      "weighted avg     1.0000    0.0227    0.0444        44\n",
      "\n",
      "Performance Metrics for Label 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     1.0000    0.9516    0.9752       248\n",
      "\n",
      "    accuracy                         0.9516       248\n",
      "   macro avg     0.3333    0.9839    0.3251       248\n",
      "weighted avg     1.0000    0.9516    0.9752       248\n",
      "\n",
      "Number of features in the training set: 41006\n",
      "Number of features in the test set: 41006\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train a Naïve Bayes classifier on the combined feature vectors\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_combined, train_ratings)\n",
    "\n",
    "# Apply the classifier to the test set\n",
    "predicted_ratings = classifier.predict(X_test_combined)\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "overall_accuracy = accuracy_score(test_ratings, predicted_ratings)\n",
    "overall_report = classification_report(test_ratings, predicted_ratings, digits=4, zero_division=1)\n",
    "\n",
    "# Calculate performance metrics for each label (1, 2, 3, 4, 5)\n",
    "label_reports = {}\n",
    "for label in range(1, 6):\n",
    "    label_indices = (test_ratings == label)\n",
    "    label_predictions = predicted_ratings[label_indices]\n",
    "    label_true_values = test_ratings[label_indices]\n",
    "    label_report = classification_report(label_true_values, label_predictions, digits=4, zero_division=1)\n",
    "    label_reports[label] = label_report\n",
    "\n",
    "# Print overall performance metrics\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
    "print('Overall Classification Report:')\n",
    "print(overall_report)\n",
    "\n",
    "# Print performance metrics for each label (1, 2, 3, 4, 5)\n",
    "for label, label_report in label_reports.items():\n",
    "    print(f'Performance Metrics for Label {label}:')\n",
    "    print(label_report)\n",
    "\n",
    "# Report the number of features in the training and test sets\n",
    "num_features_train = X_train_combined.shape[1]\n",
    "num_features_test = X_test_combined.shape[1]\n",
    "\n",
    "print(f'Number of features in the training set: {num_features_train}')\n",
    "print(f'Number of features in the test set: {num_features_test}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Add trigram features\n",
    "- Extract trigrams from the review column in the training set and add these trigrams into the feature space (unigram + bigram + trigram features). \n",
    "- Fit the new features to the review column to generate a feature vector for each sentence in the training set and testing set. Report the number of features of the training set and testing set in your documents.\n",
    "- Repeat task 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer for trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Fit and transform the training data to generate trigram feature vectors\n",
    "X_train_trigram = trigram_vectorizer.fit_transform(train_reviews)\n",
    "\n",
    "# Transform the testing data using the same trigram vectorizer\n",
    "X_test_trigram = trigram_vectorizer.transform(test_reviews)\n",
    "\n",
    "# Combine unigram, bigram, and trigram feature vectors for both training and testing sets\n",
    "X_train_combined = pd.concat([pd.DataFrame(X_train_unigram.toarray()), pd.DataFrame(X_train_bigram.toarray()), pd.DataFrame(X_train_trigram.toarray())], axis=1)\n",
    "X_test_combined = pd.concat([pd.DataFrame(X_test_unigram.toarray()), pd.DataFrame(X_test_bigram.toarray()), pd.DataFrame(X_test_trigram.toarray())], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6201\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6471    0.3826    0.4809       115\n",
      "           2     1.0000    0.0000    0.0000        26\n",
      "           3     0.0000    0.0000    0.0000        25\n",
      "           4     0.0000    0.0000    0.0000        44\n",
      "           5     0.6383    0.9677    0.7692       248\n",
      "\n",
      "    accuracy                         0.6201       458\n",
      "   macro avg     0.4571    0.2701    0.2500       458\n",
      "weighted avg     0.5649    0.6201    0.5373       458\n",
      "\n",
      "Performance Metrics for Label 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.3826    0.5535       115\n",
      "           3     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3826       115\n",
      "   macro avg     0.2500    0.8457    0.1384       115\n",
      "weighted avg     1.0000    0.3826    0.5535       115\n",
      "\n",
      "Performance Metrics for Label 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           2     1.0000    0.0000    0.0000      26.0\n",
      "           3     0.0000    1.0000    0.0000       0.0\n",
      "           4     0.0000    1.0000    0.0000       0.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      26.0\n",
      "   macro avg     0.2000    0.8000    0.0000      26.0\n",
      "weighted avg     1.0000    0.0000    0.0000      26.0\n",
      "\n",
      "Performance Metrics for Label 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           3     1.0000    0.0000    0.0000      25.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      25.0\n",
      "   macro avg     0.3333    0.6667    0.0000      25.0\n",
      "weighted avg     1.0000    0.0000    0.0000      25.0\n",
      "\n",
      "Performance Metrics for Label 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           3     0.0000    1.0000    0.0000       0.0\n",
      "           4     1.0000    0.0000    0.0000      44.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      44.0\n",
      "   macro avg     0.2500    0.7500    0.0000      44.0\n",
      "weighted avg     1.0000    0.0000    0.0000      44.0\n",
      "\n",
      "Performance Metrics for Label 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           4     0.0000    1.0000    0.0000         0\n",
      "           5     1.0000    0.9677    0.9836       248\n",
      "\n",
      "    accuracy                         0.9677       248\n",
      "   macro avg     0.3333    0.9892    0.3279       248\n",
      "weighted avg     1.0000    0.9677    0.9836       248\n",
      "\n",
      "Number of features in the training set: 97367\n",
      "Number of features in the test set: 97367\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train a Naïve Bayes classifier on the combined feature vectors\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_combined, train_ratings)\n",
    "\n",
    "# Apply the classifier to the test set\n",
    "predicted_ratings = classifier.predict(X_test_combined)\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "overall_accuracy = accuracy_score(test_ratings, predicted_ratings)\n",
    "overall_report = classification_report(test_ratings, predicted_ratings, digits=4, zero_division=1)\n",
    "\n",
    "# Calculate performance metrics for each label (1, 2, 3, 4, 5)\n",
    "label_reports = {}\n",
    "for label in range(1, 6):\n",
    "    label_indices = (test_ratings == label)\n",
    "    label_predictions = predicted_ratings[label_indices]\n",
    "    label_true_values = test_ratings[label_indices]\n",
    "    label_report = classification_report(label_true_values, label_predictions, digits=4, zero_division=1)\n",
    "    label_reports[label] = label_report\n",
    "\n",
    "# Print overall performance metrics\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
    "print('Overall Classification Report:')\n",
    "print(overall_report)\n",
    "\n",
    "# Print performance metrics for each label (1, 2, 3, 4, 5)\n",
    "for label, label_report in label_reports.items():\n",
    "    print(f'Performance Metrics for Label {label}:')\n",
    "    print(label_report)\n",
    "\n",
    "# Report the number of features in the training and test sets\n",
    "num_features_train = X_train_combined.shape[1]\n",
    "num_features_test = X_test_combined.shape[1]\n",
    "\n",
    "print(f'Number of features in the training set: {num_features_train}')\n",
    "print(f'Number of features in the test set: {num_features_test}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Add TF-IDF features\n",
    "- Among the models trained by features in tasks 3, 5 or 6 (unigram, unigram + bigram, unigram + bigram + trigram, respectively) choose the best performing model (based on overall F1 score) and substitute count-based features with TF-IDF features. \n",
    "- Repeat task 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for unigram: 0.5903\n",
      "F1 Score for unigram + bigram: 0.5499\n",
      "F1 Score for unigram + bigram + trigram: 0.5373\n",
      "The best performing model is \"unigram\" with an F1 score of 0.5903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.49      0.52       115\n",
      "           2       0.40      0.08      0.13        26\n",
      "           3       0.13      0.08      0.10        25\n",
      "           4       0.39      0.16      0.23        44\n",
      "           5       0.69      0.90      0.78       248\n",
      "\n",
      "    accuracy                           0.63       458\n",
      "   macro avg       0.44      0.34      0.35       458\n",
      "weighted avg       0.59      0.63      0.59       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#getting F1 scores\n",
    "from sklearn.metrics import f1_score\n",
    "train_reviews = train_df['review']\n",
    "train_ratings = train_df['rating']\n",
    "test_reviews = test_df['review']\n",
    "test_ratings = test_df['rating']\n",
    "\n",
    "# Initialize a list to store F1 scores for each model\n",
    "f1_scores = []\n",
    "\n",
    "for feature_set in ['unigram', 'unigram + bigram', 'unigram + bigram + trigram']:\n",
    "    if feature_set == 'unigram':\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif feature_set == 'unigram + bigram':\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    elif feature_set == 'unigram + bigram + trigram':\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    \n",
    "    \n",
    "    X_train = vectorizer.fit_transform(train_reviews)\n",
    "    X_test = vectorizer.transform(test_reviews)\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, train_ratings)\n",
    "    \n",
    "    predicted_ratings = classifier.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(test_ratings, predicted_ratings, average='weighted')\n",
    "    \n",
    "    f1_scores.append((feature_set, f1))\n",
    "\n",
    "best_model = max(f1_scores, key=lambda x: x[1])\n",
    "\n",
    "for feature_set, f1 in f1_scores:\n",
    "    print(f'F1 Score for {feature_set}: {f1:.4f}')\n",
    "\n",
    "print(f'The best performing model is \"{best_model[0]}\" with an F1 score of {best_model[1]:.4f}')\n",
    "\n",
    "if best_model[0] == 'unigram':\n",
    "    vectorizer = CountVectorizer()\n",
    "elif best_model[0] == 'unigram + bigram':\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "elif best_model[0] == 'unigram + bigram + trigram':\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "X_test = vectorizer.transform(test_reviews)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_ratings)\n",
    "predicted_ratings = classifier.predict(X_test)\n",
    "classification_rep = classification_report(test_ratings, predicted_ratings)\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since unigram performs best, we will use that for TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.5830\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6667    0.1739    0.2759       115\n",
      "           2     1.0000    0.0000    0.0000        26\n",
      "           3     1.0000    0.0000    0.0000        25\n",
      "           4     1.0000    0.0000    0.0000        44\n",
      "           5     0.5771    0.9960    0.7308       248\n",
      "\n",
      "    accuracy                         0.5830       458\n",
      "   macro avg     0.8488    0.2340    0.2013       458\n",
      "weighted avg     0.6873    0.5830    0.4650       458\n",
      "\n",
      "Performance Metrics for Label 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.1739    0.2963       115\n",
      "           5     0.0000    1.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.1739       115\n",
      "   macro avg     0.5000    0.5870    0.1481       115\n",
      "weighted avg     1.0000    0.1739    0.2963       115\n",
      "\n",
      "Performance Metrics for Label 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           2     1.0000    0.0000    0.0000      26.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      26.0\n",
      "   macro avg     0.3333    0.6667    0.0000      26.0\n",
      "weighted avg     1.0000    0.0000    0.0000      26.0\n",
      "\n",
      "Performance Metrics for Label 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           3     1.0000    0.0000    0.0000      25.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      25.0\n",
      "   macro avg     0.3333    0.6667    0.0000      25.0\n",
      "weighted avg     1.0000    0.0000    0.0000      25.0\n",
      "\n",
      "Performance Metrics for Label 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000       0.0\n",
      "           4     1.0000    0.0000    0.0000      44.0\n",
      "           5     0.0000    1.0000    0.0000       0.0\n",
      "\n",
      "    accuracy                         0.0000      44.0\n",
      "   macro avg     0.3333    0.6667    0.0000      44.0\n",
      "weighted avg     1.0000    0.0000    0.0000      44.0\n",
      "\n",
      "Performance Metrics for Label 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    1.0000    0.0000         0\n",
      "           5     1.0000    0.9960    0.9980       248\n",
      "\n",
      "    accuracy                         0.9960       248\n",
      "   macro avg     0.5000    0.9980    0.4990       248\n",
      "weighted avg     1.0000    0.9960    0.9980       248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_reviews)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_reviews)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, train_ratings)\n",
    "\n",
    "predicted_ratings = classifier.predict(X_test_tfidf)\n",
    "\n",
    "overall_accuracy = accuracy_score(test_ratings, predicted_ratings)\n",
    "overall_report = classification_report(test_ratings, predicted_ratings, digits=4, zero_division=1)\n",
    "\n",
    "# Calculate performance metrics for each label (1, 2, 3, 4, 5)\n",
    "label_reports = {}\n",
    "for label in range(1, 6):\n",
    "    label_indices = (test_ratings == label)\n",
    "    label_predictions = predicted_ratings[label_indices]\n",
    "    label_true_values = test_ratings[label_indices]\n",
    "    label_report = classification_report(label_true_values, label_predictions, digits=4, zero_division=1)\n",
    "    label_reports[label] = label_report\n",
    "\n",
    "# Print overall performance metrics\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
    "print('Overall Classification Report:')\n",
    "print(overall_report)\n",
    "\n",
    "# Print performance metrics for each label (1, 2, 3, 4, 5)\n",
    "for label, label_report in label_reports.items():\n",
    "    print(f'Performance Metrics for Label {label}:')\n",
    "    print(label_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8:  Train models with other columns\n",
    "- Among the models trained by features in tasks 3, 5, 6 or 7, choose the best performing feature set, and train Naïve Bayes classifiers on the following columns: (1) title, (2) title + review. Record the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'title', 'review', 'rating'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using \"title\" column: 0.6135\n",
      "Classification Report using \"title\" column:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6557    0.3478    0.4545       115\n",
      "           2     1.0000    0.0000    0.0000        26\n",
      "           3     0.1429    0.0400    0.0625        25\n",
      "           4     0.2857    0.1364    0.1846        44\n",
      "           5     0.6341    0.9435    0.7585       248\n",
      "\n",
      "    accuracy                         0.6135       458\n",
      "   macro avg     0.5437    0.2935    0.2920       458\n",
      "weighted avg     0.6000    0.6135    0.5460       458\n",
      "\n",
      "Accuracy using \"title + review\" column: 0.6419\n",
      "Classification Report using \"title + review\" column:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5758    0.4957    0.5327       115\n",
      "           2     0.2857    0.0769    0.1212        26\n",
      "           3     0.0909    0.0800    0.0851        25\n",
      "           4     0.4762    0.2273    0.3077        44\n",
      "           5     0.7217    0.8992    0.8007       248\n",
      "\n",
      "    accuracy                         0.6419       458\n",
      "   macro avg     0.4301    0.3558    0.3695       458\n",
      "weighted avg     0.6023    0.6419    0.6084       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_reviews = train_df['review']\n",
    "train_ratings = train_df['rating']\n",
    "test_reviews = test_df['review']\n",
    "test_ratings = test_df['rating']\n",
    "train_titles = train_df['title']\n",
    "test_titles = test_df['title']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_titles = vectorizer.fit_transform(train_titles)\n",
    "X_test_titles = vectorizer.transform(test_titles)\n",
    "\n",
    "X_train_combined = vectorizer.fit_transform(train_titles + ' ' + train_reviews)\n",
    "X_test_combined = vectorizer.transform(test_titles + ' ' + test_reviews)\n",
    "\n",
    "\n",
    "classifier_title = MultinomialNB()\n",
    "classifier_title_review = MultinomialNB()\n",
    "\n",
    "\n",
    "classifier_title.fit(X_train_titles, train_ratings)\n",
    "\n",
    "classifier_title_review.fit(X_train_combined, train_ratings)\n",
    "\n",
    "predicted_ratings_title = classifier_title.predict(X_test_titles)\n",
    "predicted_ratings_title_review = classifier_title_review.predict(X_test_combined)\n",
    "\n",
    "#eval\n",
    "accuracy_title = accuracy_score(test_ratings, predicted_ratings_title)\n",
    "accuracy_title_review = accuracy_score(test_ratings, predicted_ratings_title_review)\n",
    "\n",
    "report_title = classification_report(test_ratings, predicted_ratings_title, digits=4, zero_division=1)\n",
    "report_title_review = classification_report(test_ratings, predicted_ratings_title_review, digits=4, zero_division=1)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy using \"title\" column: {accuracy_title:.4f}')\n",
    "print('Classification Report using \"title\" column:')\n",
    "print(report_title)\n",
    "\n",
    "print(f'Accuracy using \"title + review\" column: {accuracy_title_review:.4f}')\n",
    "print('Classification Report using \"title + review\" column:')\n",
    "print(report_title_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples misclassified as label 1:\n",
      "True Rating: 1, Predicted Rating: 5\n",
      "Review Text: Fix it\n",
      "\n",
      "True Rating: 1, Predicted Rating: 5\n",
      "Review Text: Old version of ChatGPT.\n",
      "\n",
      "True Rating: 1, Predicted Rating: 3\n",
      "Review Text: Unusable on iPad Pro currently.\n",
      "\n",
      "Examples misclassified as label 2:\n",
      "True Rating: 2, Predicted Rating: 1\n",
      "Review Text: cant login\n",
      "\n",
      "True Rating: 2, Predicted Rating: 1\n",
      "Review Text: i subscribe the plus for gpt4???but just get gpt3.0 with fake label 4.0.release real gpt4 now?????????\n",
      "\n",
      "True Rating: 2, Predicted Rating: 3\n",
      "Review Text: This app is based on chatgpt 3 (cutoff date Sep 2021)\n",
      "\n",
      "Examples misclassified as label 3:\n",
      "True Rating: 3, Predicted Rating: 1\n",
      "Review Text: ????\n",
      "\n",
      "True Rating: 3, Predicted Rating: 1\n",
      "Review Text: Ok\n",
      "\n",
      "True Rating: 3, Predicted Rating: 1\n",
      "Review Text: ipad is not supported\n",
      "\n",
      "Examples misclassified as label 4:\n",
      "True Rating: 4, Predicted Rating: 5\n",
      "Review Text: First\n",
      "\n",
      "True Rating: 4, Predicted Rating: 5\n",
      "Review Text: I am in love with CHATGPT\n",
      "\n",
      "True Rating: 4, Predicted Rating: 5\n",
      "Review Text: great!\n",
      "\n",
      "Examples misclassified as label 5:\n",
      "True Rating: 5, Predicted Rating: 3\n",
      "Review Text: We need IPadOS!!!\n",
      "\n",
      "True Rating: 5, Predicted Rating: 1\n",
      "Review Text: ???????????????????????????plus???????????€????????????????????????????????????????????????????????????????????€?????????????????????????????????????????????\n",
      "\n",
      "True Rating: 5, Predicted Rating: 1\n",
      "Review Text: H??y b??? sung v??ng Vi???t Nam ????? t??i c?? th??? mua g??i plus v?? kh??ng ph???i chuy???n v??ng qua M??? ????? t???i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming `predicted_ratings_title_review` contains the predictions of the best-performing model\n",
    "\n",
    "misclassified_indices = [i for i, (true_rating, predicted_rating) in enumerate(zip(test_ratings, predicted_ratings_title_review)) if true_rating != predicted_rating]\n",
    "\n",
    "# Initialize a dictionary to store misclassified examples for each label\n",
    "misclassified_examples = {label: [] for label in range(1, 6)}\n",
    "\n",
    "# Populate the dictionary with misclassified examples\n",
    "for index in misclassified_indices:\n",
    "    true_rating = test_ratings[index]\n",
    "    predicted_rating = predicted_ratings_title_review[index]\n",
    "    review_text = test_reviews.iloc[index]\n",
    "    \n",
    "    # Add the misclassified example to the corresponding label\n",
    "    misclassified_examples[true_rating].append((review_text, predicted_rating))\n",
    "\n",
    "# Print 3 examples for each label\n",
    "for label in range(1, 6):\n",
    "    print(f\"Examples misclassified as label {label}:\")\n",
    "    for example in misclassified_examples[label][:3]:\n",
    "        print(f\"True Rating: {label}, Predicted Rating: {example[1]}\")\n",
    "        print(f\"Review Text: {example[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three-Way Classification Results:\n",
      "Accuracy: 0.7358\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6881    0.5319    0.6000       141\n",
      "     Neutral     0.1667    0.0800    0.1081        25\n",
      "    Positive     0.7715    0.8904    0.8267       292\n",
      "\n",
      "    accuracy                         0.7358       458\n",
      "   macro avg     0.5421    0.5008    0.5116       458\n",
      "weighted avg     0.7128    0.7358    0.7177       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_to_three_way_labels(labels):\n",
    "    mapped_labels = []\n",
    "    for label in labels:\n",
    "        if label in [1, 2]:\n",
    "            mapped_labels.append('Negative')\n",
    "        elif label == 3:\n",
    "            mapped_labels.append('Neutral')\n",
    "        elif label in [4, 5]:\n",
    "            mapped_labels.append('Positive')\n",
    "    return mapped_labels\n",
    "\n",
    "train_three_way_labels = map_to_three_way_labels(train_ratings)\n",
    "test_three_way_labels = map_to_three_way_labels(test_ratings)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "X_test = vectorizer.transform(test_reviews)\n",
    "\n",
    "classifier_three_way = MultinomialNB()\n",
    "\n",
    "classifier_three_way.fit(X_train, train_three_way_labels)\n",
    "\n",
    "\n",
    "predicted_three_way_labels = classifier_three_way.predict(X_test)\n",
    "accuracy_three_way = accuracy_score(test_three_way_labels, predicted_three_way_labels)\n",
    "report_three_way = classification_report(test_three_way_labels, predicted_three_way_labels, digits=4)\n",
    "\n",
    "print('Three-Way Classification Results:')\n",
    "print(f'Accuracy: {accuracy_three_way:.4f}')\n",
    "print('Classification Report:')\n",
    "print(report_three_way)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - \n",
    "The write-ups will be in the word document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38566582ccfda89db93d53beb5169dd33b51eb8349bf2ac29ed4a79884e15a79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
